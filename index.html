<!doctype html>
<html lang="en">

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-21408087-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-21408087-2');
    </script>

    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <title>Gege's Website</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="manifest" href="site.webmanifest">
    <link rel="apple-touch-icon" href="icon.png">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="css/style_v2.css">
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
    <!-- <link rel="icon" href="favicon.ico"> -->
</head>

<body>
    <div class="wrapper">
        <nav id="sidebar">
            <div id="sidebar-wrapper">
                <div class="sidebar-header">
                    <h1><span class="first-name">Gege</span> <span class="last-name">Gao</span></h1>
                    <p>Personal Website</p>
                </div>

                <ul class="list-unstyled components">
                     <li class="active" > <a href="index.html">About</a> </li> 
                </ul>
                <ul id="links" class="list-unstyled">
                    <li><a class="mail">
                            <svg style="width:20px;height:20px" class="mr-1" viewBox="0 0 24 24">
                                <path fill="currentColor" d="M20,8L12,13L4,8V6L12,11L20,6M20,4H4C2.89,4 2,4.89 2,6V18A2,2 0 0,0 4,20H20A2,2 0 0,0 22,18V6C22,4.89 21.1,4 20,4Z" />
                            </svg>
                            gege.gao at inf.ethz.ch
                        </a>
                    </li>
                    <li><a href="https://scholar.google.com/citations?user=nYYIYaUAAAAJ">
                            <svg style="width:20px;height:20px" class="mr-1" viewBox="0 0 24 24">
                                <path fill="currentColor" d="M12,3L1,9L12,15L21,10.09V17H23V9M5,13.18V17.18L12,21L19,17.18V13.18L12,17L5,13.18Z" />
                            </svg>
                            Google Scholar
                        </a>
                    </li>
                    <li><a href="https://twitter.com/GegeHeseri">
                            <svg style="width:20px;height:20px" class="mr-1" viewBox="0 0 24 24">
                                <path fill="currentColor" d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" />
                            </svg>
                            Twitter
                        </a>
                    </li>
                    <li> <a href="https://github.com/GGGHSL">
                            <svg style="width:20px;height:20px" class="mr-1" viewBox="0 0 24 24">
                                <path fill="currentColor" d="M12,2A10,10 0 0,0 2,12C2,16.42 4.87,20.17 8.84,21.5C9.34,21.58 9.5,21.27 9.5,21C9.5,20.77 9.5,20.14 9.5,19.31C6.73,19.91 6.14,17.97 6.14,17.97C5.68,16.81 5.03,16.5 5.03,16.5C4.12,15.88 5.1,15.9 5.1,15.9C6.1,15.97 6.63,16.93 6.63,16.93C7.5,18.45 8.97,18 9.54,17.76C9.63,17.11 9.89,16.67 10.17,16.42C7.95,16.17 5.62,15.31 5.62,11.5C5.62,10.39 6,9.5 6.65,8.79C6.55,8.54 6.2,7.5 6.75,6.15C6.75,6.15 7.59,5.88 9.5,7.17C10.29,6.95 11.15,6.84 12,6.84C12.85,6.84 13.71,6.95 14.5,7.17C16.41,5.88 17.25,6.15 17.25,6.15C17.8,7.5 17.45,8.54 17.35,8.79C18,9.5 18.38,10.39 18.38,11.5C18.38,15.32 16.04,16.16 13.81,16.41C14.17,16.72 14.5,17.33 14.5,18.26C14.5,19.6 14.5,20.68 14.5,21C14.5,21.27 14.66,21.59 15.17,21.5C19.14,20.16 22,16.42 22,12A10,10 0 0,0 12,2Z" />
                            </svg>
                            GGGHSL</a>
                    </li>
                    <!-- <li> <a href="https://linkedin.com/in/sx-yu"> -->
                    <!--         <svg style="width:20px;height:20px" viewBox="0 0 24 24"> -->
                    <!--             <path fill="currentColor" d="M19 3A2 2 0 0 1 21 5V19A2 2 0 0 1 19 21H5A2 2 0 0 1 3 19V5A2 2 0 0 1 5 3H19M18.5 18.5V13.2A3.26 3.26 0 0 0 15.24 9.94C14.39 9.94 13.4 10.46 12.92 11.24V10.13H10.13V18.5H12.92V13.57C12.92 12.8 13.54 12.17 14.31 12.17A1.4 1.4 0 0 1 15.71 13.57V18.5H18.5M6.88 8.56A1.68 1.68 0 0 0 8.56 6.88C8.56 5.95 7.81 5.19 6.88 5.19A1.69 1.69 0 0 0 5.19 6.88C5.19 7.81 5.95 8.56 6.88 8.56M8.27 18.5V10.13H5.5V18.5H8.27Z" /> -->
                    <!--         </svg> -->
                    <!--         sx-yu -->
                    <!--     </a> -->
                </ul>
            </div>
        </nav>

        <div id="content">
            
    <div class="about">
        <img class="banner-image" src="img/banner_2023.jpeg" title="PhD Time">
        <p style="text-align: center;">
            <b>Figure 1.</b> Visual records of Gege's workplaces during her PhD journey in Zürich and Tübingen. 
        </p>
        <br>
        <br>
        <h2><b>Bio</b></h2>
        <!-- <img class="photo" src="img/photo.png"> -->
        <!-- width="100" height="100" -->
        <p>
            Gege Gao (高格格) is a Ph.D. student in Computer Science since November 2022, advised by <a href="https://is.mpg.de/~bs">Prof. Dr. Bernhard Schölkopf</a> at ETH Zürich and Max Planck Institute for Intelligent Systems (Tübingen) and <a href="https://ps.is.mpg.de/~ageiger">Prof. Dr. Andreas Geiger</a> at University of Tübingen and Tübingen AI Center, as shown in Figure 1. Before that, she was a Research Staff at Institute of Automation, Chinese Academy of Sciences since 2020. She received her master's degree of M.Sc. in Applied Statistics from Renmin University of China in 2020, supervised by <a href="http://stat.ruc.edu.cn/Home/People/Faculty/8e5ad6f548314fc1be75429b60164f7b.htm">Prof. Dr. Xiaoling Lu</a>. Her research interests include controllable content creation, inverse graphics, and representation learning and reasoning.
        </p>
    </div>
    <!-- <div class="timeline pt-2">
        
        <div class="timeline-entry">
            <div class="timeline-image-holder">
            
            <img src="img/timeline/ETH-D-INFK.png" alt="ETH Zürich">
            
            </div>
            <div class="timeline-time">November 2022</div>
            <div class="timeline-org">ETH Zürich</div>
            <div class="timeline-position">D-INFK</div>
            <div class="timeline-description">Supervisor: Prof. Dr. Bernhard Schölkopf</div>
        </div>
        
        <div class="timeline-entry">
            <div class="timeline-image-holder">
            
            <img src="img/timeline/mpi.png" alt="MPI-IS (Tübingen)">
            
            </div>
            <div class="timeline-time">November 2022</div>
            <div class="timeline-org">MPI-IS (Tübingen)</div>
            <div class="timeline-position">Empirical Inference</div>
            <div class="timeline-description"></div>
        </div>
        
        <div class="timeline-entry">
            <div class="timeline-image-holder">
            
            <img src="img/timeline/avg.svg" alt="University of Tübingen, Tübingen AI Center">
            
            </div>
            <div class="timeline-time">November 2022</div>
            <div class="timeline-org">University of Tübingen, Tübingen AI Center</div>
            <div class="timeline-position">Autonomous Vision Group</div>
            <div class="timeline-description">Supervisor: Prof. Dr. Andreas Geiger</div>
        </div>
        
    </div> -->
    <div class="pt-4">
        <h2><b>Publications</b></h2>
        <div id="pubs">
            
            <div class="pub row pt-3 pb-3">
                <div class="col-sm-2 pub-preview">
                    
                    <img src="img/graphdreamer.jpg" alt="GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs">
                    
                </div>
                <div class="col">
                    <h4>GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs</h4>
                    <div class="pub-authors">
                        
                        <a href="https://ggghsl.github.io/" class="link-self">Gege Gao</a>, 
                        
                        <a href="https://wyliu.com/">Weiyang Liu</a>, 
                        
                        <a href="https://apchenstu.github.io/">Anpei Chen</a>, 
                        
                        <a href="https://apchenstu.github.io/">Andreas Geiger</a>, 
                        
                        <a href="https://is.mpg.de/~bs">Bernhard Schölkopf</a>
                        
                    </div>
                    <div class="pub-venue pb-1">
                        <small>
                        
                        <span class="pub-award">  </span>
                        </small>
                    </div>
                    <div class="pub-links">
                    
                    <a href="https://arxiv.org/abs/2312.00093">arXiv</a>
                    
                    
                    <a href="https://arxiv.org/pdf/2312.00093.pdf">Paper</a>
                    
                    
                    <a href="https://graphdreamer.github.io">Website</a>
                    
                    
                    
                    </div>
                    <div class="pub-description pt-1">
                        
                        <p class="pub-abstract">
                        As pretrained text-to-image diffusion models become increasingly powerful, recent efforts have been made to distill knowledge from these text-to-image pretrained models for optimizing a text-guided 3D model. Most of the existing methods generate a holistic 3D model from a plain text input. This can be problematic when the text describes a complex scene with multiple objects, because the vectorized text embeddings are inherently unable to capture a complex description with multiple entities and relationships. Holistic 3D modeling of the entire scene further prevents accurate grounding of text entities and concepts. To address this limitation, we propose GraphDreamer, a novel framework to generate compositional 3D scenes from scene graphs, where objects are represented as nodes and their interactions as edges. By exploiting node and edge information in scene graphs, our method makes better use of the pretrained text-to-image diffusion model and is able to fully disentangle different objects without image-level supervision. To facilitate modeling of object-wise relationships, we use signed distance fields as representation and impose a constraint to avoid inter-penetration of objects. To avoid manual scene graph creation, we design a text prompt for ChatGPT to generate scene graphs based on text inputs. We conduct both qualitative and quantitative experiments to validate the effectiveness of GraphDreamer in generating high-fidelity compositional 3D scenes with disentangled object entities.
                        </p>
                        
                    </div>
                </div>
            </div>
            
            <div class="pub row pt-3 pb-3">
                <div class="col-sm-2 pub-preview">
                    
                    <img src="img/causal.png" alt="Causal Representation Learning for Context-Aware Face Transfer">
                    
                </div>
                <div class="col">
                    <h4>Causal Representation Learning for Context-Aware Face Transfer</h4>
                    <div class="pub-authors">
                        
                        <a href="https://ggghsl.github.io/" class="link-self">Gege Gao</a>, 
                        
                        Huaibo Huang, 
                        
                        Chaoyou Fu, 
                        
                        Ran He
                        
                    </div>
                    <div class="pub-venue pb-1">
                        <small>
                        
                        <span class="pub-award">  </span>
                        </small>
                    </div>
                    <div class="pub-links">
                    
                    <a href="https://arxiv.org/abs/2110.01571">arXiv</a>
                    
                    
                    
                    
                    
                    </div>
                    <div class="pub-description pt-1">
                        
                        <p class="pub-abstract">
                        Human face synthesis involves transferring knowledge about the identity and identity-dependent shape of a human face to target face images where the context (e.g., facial expressions, head poses, and other background factors) may change dramatically. Human faces are non-rigid, so facial expression leads to deformation of face shape, and head pose also affects the face observed in 2D images. A key challenge in face transfer is to match the face with unobserved new contexts, adapting the identity-dependent face shape (IDFS) to different poses and expressions accordingly. In this work, we find a way to provide prior knowledge for generative models to reason about the appropriate appearance of a human face in response to various expressions and poses. We propose a novel context-aware face transfer model, called CarTrans, that incorporates causal effects of contextual factors into face representation, and thus is able to be aware of the uncertainty of new contexts. We estimate the effect of facial expression and head pose in terms of counterfactuals by designing a controlled intervention trial, thus avoiding the need for dense multi-view observations to cover the pose-expression space well. Moreover, we propose a kernel regression-based encoder that eliminates the identity specificity of the target face when encoding contextual information from the target image. The resulting method shows impressive performance, allowing fine-grained control over face shape and appearance under various contextual conditions.
                        </p>
                        
                    </div>
                </div>
            </div>
            
            <div class="pub row pt-3 pb-3">
                <div class="col-sm-2 pub-preview">
                    
                    <img src="img/infoswap.png" alt="Information Bottleneck Disentanglement for Identity Swapping">
                    
                </div>
                <div class="col">
                    <h4>Information Bottleneck Disentanglement for Identity Swapping</h4>
                    <div class="pub-authors">
                        
                        <a href="https://ggghsl.github.io/" class="link-self">Gege Gao</a>, 
                        
                        Huaibo Huang, 
                        
                        Chaoyou Fu, 
                        
                        Zhaoyang Li, 
                        
                        Ran He
                        
                    </div>
                    <div class="pub-venue pb-1">
                        <small>
                        CVPR 2021
                        <span class="pub-award">  </span>
                        </small>
                    </div>
                    <div class="pub-links">
                    
                    
                    <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Gao_Information_Bottleneck_Disentanglement_for_Identity_Swapping_CVPR_2021_paper.html">Paper</a>
                    
                    
                    
                    <a href="https://github.com/GGGHSL/InfoSwap-master">Code</a>
                    
                    
                    </div>
                    <div class="pub-description pt-1">
                        
                        <p class="pub-abstract">
                        Improving the performance of face forgery detectors often requires more identity-swapped images of higher-quality. One core objective of identity swapping is to generate identity-discriminative faces that are distinct from the target while identical to the source. To this end, properly disentangling identity and identity-irrelevant information is critical and remains a challenging endeavor. In this work, we propose a novel information disentangling and swapping network, called InfoSwap, to extract the most expressive information for identity representation from a pre-trained face recognition model. The key insight of our method is to formulate the learning of disentangled representations as optimizing an information bottleneck trade-off, in terms of finding an optimal compression of the pre-trained latent features. Moreover, a novel identity contrastive loss is proposed for further disentanglement by requiring a proper distance between the generated identity and the target. While the most prior works have focused on using various loss functions to implicitly guide the learning of representations, we demonstrate that our model can provide explicit supervision for learning disentangled representations, achieving impressive performance in generating more identity-discriminative swapped faces.
                        </p>
                        
                    </div>
                </div>
            </div>
            
            <!-- <small>
            * = Equal contribution
            </small> -->
        </div>
    </div>
    <div class="coursework pt-4">
        <h2><b>Education</b></h2>
        <div class="timeline pt-2">
            
            <div class="timeline-entry">
                <div class="timeline-image-holder">
                
                <img src="img/timeline/ETH-D-INFK.png" alt="ETH Zürich">
                
                </div>
                <div class="timeline-time">November 2022</div>
                <div class="timeline-org">ETH Zürich</div>
                <div class="timeline-position">D-INFK</div>
                <div class="timeline-description">Supervisor: Prof. Dr. Bernhard Schölkopf</div>
            </div>
            
            <div class="timeline-entry">
                <div class="timeline-image-holder">
                
                <img src="img/timeline/mpi.png" alt="MPI-IS (Tübingen)">
                
                </div>
                <div class="timeline-time">November 2022</div>
                <div class="timeline-org">MPI-IS (Tübingen)</div>
                <div class="timeline-position">Empirical Inference</div>
                <div class="timeline-description"></div>
            </div>
            
            <div class="timeline-entry">
                <div class="timeline-image-holder">
                
                <img src="img/timeline/avg.svg" alt="University of Tübingen, Tübingen AI Center">
                
                </div>
                <div class="timeline-time">November 2022</div>
                <div class="timeline-org">University of Tübingen, Tübingen AI Center</div>
                <div class="timeline-position">Autonomous Vision Group</div>
                <div class="timeline-description">Supervisor: Prof. Dr. Andreas Geiger</div>
            </div>
            
        </div>
        <div id="coursework-collapse">
            <h4><b>M.Sc.</b> in Applied Statistics, 2020</h4>
<ul>
    <li>School of Statistics</li>
    <li>Renmin University of China</li>
</ul>
<h4><b>B.Sc.</b> in Applied Statistics, 2017</h4>
<ul>
    <li>School of Mathematics and Statistics</li>
    <li>Central University of Finance and Economics</li>
</ul>
        </div>
    </div>
    <!-- <div class="coursework pt-2">
        <h2>Misc</h2>
        <p>
            I was born in Beijing, China. My Chinese name is Gege Gao (高格格). Hešeri (赫舍里) is my ancestral surname.
        </p>
    </div> -->
    <div class="pt-2">
        <small>
            The source code for this website is forked from <a href="https://github.com/sxyu/sxyu.github.io">this repo</a>.
        </small>
    </div>

        </div>
    </div>
</body>


<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</html>